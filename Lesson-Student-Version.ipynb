{"cells": [{"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Resampling methods\n", "\n", "Resampling techniques are modern statistical methods that involve taking repeated subsamples from a sample. \n", "\n", "The purpose of resampling methods is to increase the size of our samples without actually having to obtain more samples. \n", "\n", "We will discuss two resampling methods, permutation tests and the bootstrap. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Permutation test\n", "\n", "You have seen how to test whether the difference between two samples is statistically significant using independent samples t-tests. T-tests are parametric tests that assume a known form for the distribution of the test statistic under the null hypothesis. For this reason, to use t-tests you have to assume that that samples are normally distributed. \n", "\n", "What if you want to know if two samples come from the same population or not, but you do not know how the samples are distributed? \n", "\n", "We use permutation tests when we want to know if two samples are from the same distribution but we cannot make the assumption that the samples are normally distributed. A permutation test is a type of statistical significance test that builds the sampling distribution of the test statistic under the null hypothesis by calculating all possible values of the test statistic from all possible reshufflings of the labels of the observed data points.\n", "\n", "Permutation tests are a type of non-parametric test. No assumptions are made about the sampling distribution of the test statistic under the null hypothesis.\n", "\n", "Let's work together on an example."]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Example: \n", "\n", "Suppose we collect two samples of scores, `scores_A` and `scores_B`, of students' performance on a test for students belonging to two different classrooms, classroom A and classroom B. We want to know whether `scores_A` and `scores_B` come from the same distribution.\n", "\n", "``` python\n", "scores_A = [87.8, 93.8, 67.2, 82.0, 87.2, 78.0, 78.7, 82.2]\n", "scores_B = [97.8, 83.4, 88.2, 92.3, 103.0, 124.4, 98.4, 74.8, 73.1]\n", "``` "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["First, we create two lists, `scores_A` and `scores_B`, that will hold the values for the test scores for students in classroom A and B, respectively. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["scores_A = [87.8, 93.8, 67.2, 82.0, 87.2, 78.0, 78.7, 82.2]\n", "scores_B = [97.8, 83.4, 88.2, 92.3, 103.0, 124.4, 98.4, 74.8, 73.1]"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["**What is the null hypothesis we want to test? What is the alternative hypothesis?**"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["You'll to use a permutation test to assess whether the difference between the sample means is different enough (larger or smaller) to reject the null hypothesis that both samples come from the same distribution, at a significance level of $\\alpha = 0.05$. \n", "\n", "## Let's write some code!\n", "\n", "In this example, we'll take the test statistic to be the difference in sample means. \n", "\n", "Since we've chosen the test statistic to be the difference in means, you'll write a function to compute the difference in means for two samples. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["def difference_in_means(sample1, sample2):\n", "\n", "    pass"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Use the function to compute the observed samples' difference in means, and store this value in a variable called `observed_differences`. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import numpy as np \n", "\n", "# Compute the difference in means between the two observed samples of scores\n", "observed_difference = None\n", "print(observed_difference)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["If the null hypothesis is correct, both samples come from the same distribution, and a single sample containing all of the observations from `scores_A` and `scores_B` would just be a sample with larger sample size from the same distribution. \n", "\n", "Pool the observations from `scores_A` and `scores_B` into one single combined sample. Call this sample `combined_scores`. \n", "\n", "_Hint: Use `np.concatenate`._"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["combined_scores = None"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["If the null hypothesis is true, each observed score is equally likely to be part of either sample of scores. \n", "\n", "We need to create all possible \"reshufflings\" of the samples to check the distribution of the test statistic under the null hypothesis to then perform our hypothesis test. \n", "\n", "**How many possible ways can we split the combined scores to produce two samples with sizes equal to the sizes of the original samples?**\n", "\n", "_Hint: `scipy.special.comb(N, k)` computes the number of combinations of N things taken k at a time._"]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from scipy.special import comb\n", "\n", "# your code here "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Next, we compute the difference in sample means for all possible regroupings of the samples. \n", "\n", "In the cell below, we initialize an empty list to hold all the values of the test statistic computed on the reshuffled samples, and compute the difference of the means for each permuted set of samples: "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["from itertools import combinations\n", "\n", "# initialize an empty list to hold all values of the test statistic computed on the permuted samples\n", "diff_means = []\n", "\n", "for a_ in combinations(combined_scores, n_A): \n", "    b_ = list(combined_scores.copy())\n", "    for i in a_:\n", "        b_.remove(i)\n", "    mean_a_ = np.mean(a_)\n", "    mean_b_ = np.mean(b_)\n", "    \n", "    diff = mean_a_ - mean_b_\n", "    diff_means.append(diff)\n", "\n", "diff_means = np.array(diff_means)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Create a histogram to show the distribution of the test statistic under the null hypothesis. Use `bins = 20`. \n", "\n", "Add a vertical line showing the observed test statistic using `plt.axvline`. Set the color of this vertical line to black using `color='k'`. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["import matplotlib.pyplot as plt \n", "%matplotlib inline \n", "\n", "# your code here \n"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Now that you have all the differences in sample means for all possible regroupings of the samples, count the number of times that we obtained a difference in means for the reshuffled samples as extreme as the observed difference in sample means. \n", "\n", "_Hint: What was the alternative hypothesis? Should we run a one-sided or a two-sided test?_ "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# your code here "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["The probability of obtaining a test statistic as extreme as the observed test statistic, the p-value of the test, is given by the ratio of this number to the total number of reshufflings of the data. \n", "\n", "Compute the p-value of the test. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# your code here "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["You just performed a permutation test to assess if the samples of scores from classrooms A and B suggested that both samples came from the same distribution.\n", "\n", "What can you conclude from your result? "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["How does your result compare to a two-sample t-test? \n", "\n", "_Hint: Use scipy.stats._ "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# your code here "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Monte Carlo Simulation: Permutation Test\n", "\n", "In the past example, there were 24,310 possible permutations of the samples of scores, with original sample sizes of 8 and 9. \n", "\n", "As sample sizes increase, the number of possible permutations of the samples increase too, in which case it becomes too computationally expensive to compute an exact permutation test. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Imagine that we have two samples of relative humidity measurements performed at two different locations, location 1 and location 2, and we wanted to assess whether the samples came from the same distribution or not. Sample 1 has 20 observations and sample 2 has 25 observations. \n", "\n", "```python\n", "sample1 = [39.7, 57.12, 64.76, 38.66, 56.36, 55.71, 41.91, 61.8, 53.63, 38.55, 46.28, 43.49, 52.32, 41.43, 49.95, 46.7, 60.8, 44.44, 46.65, 60.23]\n", "\n", "sample2 = [55.77, 56.88, 39.31, 44.38, 43.9, 36.65, 45.47, 61.74, 49.82, 33.45, 56.89, 40.96, 31.46, 34.2, 42.8, 54.92, 50.08, 48.79, 54.58, 30.77, 63.92, 43.54, 31.35, 37.42, 59.93]\n", "```\n", "\n", "Let's set our null hypothesis and alternative hypothesis: \n", "\n", "* The null hypothesis is that there is no difference i the relative humidity of location 1 and location 2. \n", "* The alternative hypothesis is that location 1 is more humid than location 2. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["If we wanted to run an exact permutation test in this case, we would need to compute over 3 TRILLION calculations of the test statistic! This is not computationally feasible! \n", "\n", "To get around this, we can _emulate_ a permutation test using Monte Carlo simulations to sample from the sample space of all possible permutations constructed from the original samples. \n", "\n", "Write a Monte Carlo simulation to sample from the permutation space. \n", "* To do this, we'll follow similar steps as above but, instead of computing the test statistic for all possible permutations of the samples, we'll pick a more suitable number of simulated permuted samples, say, 10,000 permuted samples. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["sample1 = [39.7, 57.12, 64.76, 38.66, 56.36, 55.71, 41.91, 61.8, 53.63, 38.55, 46.28, \n", "           43.49, 52.32, 41.43, 49.95, 46.7, 60.8, 44.44, 46.65, 60.23]\n", "sample2 = [55.77, 56.88, 39.31, 44.38, 43.9, 36.65, 45.47, 61.74, 49.82, 33.45, 56.89, \n", "           40.96, 31.46, 34.2, 42.8, 54.92, 50.08, 48.79, 54.58, 30.77, 63.92, 43.54,\n", "           31.35, 37.42, 59.93]"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's pick the test statistic to be, once again for simplicity, the difference of the means. \n", "\n", "**What's the observed test statistic?** \n", "_Hint: Use the function you coded beforehand._ "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["observed_difference = None"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Let's run a Monte Carlo simulation of the permutation test using just 10,000 simulations of permuted samples.\n", "\n", "The code cell below has starter code which should help along the way."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["n_simulations = 10**4\n", "\n", "# Create an array to store all the test statistics from the permuted samples\n", "diffs = np.empty(n_simulations)\n", "\n", "# Create the combined sample from sample1 and sample2\n", "combined_sample = None\n", "\n", "# Set a random seed for reproducibility\n", "np.random.seed(42)\n", "\n", "# for each of the 10000 simulations, compute the test statistic and place in the array\n", "for i in range(n_simulations):\n", "    \n", "    # Permute the order of the combined_sample\n", "    permuted_combined_sample = np.random.permutation(None)\n", "\n", "    # Split the permuted combined sample into two samples of size len(sample1) and len(sample2)\n", "    permuted_sample1 = None\n", "    permuted_sample2 = None \n", "    \n", "    # Compute the test statistic for the permuted samples \n", "    diff = None \n", "    \n", "    # Store the result in the array\n", "    diffs[i] = diff"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Compute the p-value. Recall that the probability of obtaining a test statistic as extreme as the observed test statistic, the p-value of the test, is given by the ratio of the number of times we obtained a test statistic from our permuted samples as extreme as the observed test statistic to the number of simulations ran. \n", "\n", "_Hint: What as the alternative hypothesis? Should we set a one-sided or a two-sided test?_ "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# your code here "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["What happens to the p-value if you run 50,000 or 100,000 simulations instead? "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Bootstrapping and Hypothesis testing"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Bootstrapping is another resampling technique that we can use to perform statistical tests.\n", "\n", "A bootstrap sample is a sample that has been obtained by sampling with replacement from an original sample. \n", "\n", "For example, if our original sample is `s = [1, 2, 3, 4, 5]`, the sample `s_bootstrap = [1, 1, 1, 1, 1]` is a possible bootstrap sample from s. Nothing stops the observation with value 1 to be sampled five repeated times, as sampling is performed with replacement. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["To create a bootstrap sample of equal size to the original sample,  use `np.random.choice` and set the parameters `replace = True` and `size = len(original_sample)`. \n", "\n", "Here's an example that shows how to create three same-sized bootstrap samples of the `scores_A` sample defined above: "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["scores_A = [87.8, 93.8, 67.2, 82.0, 87.2, 78.0, 78.7, 82.2]\n", "\n", "# set a seed for reproducibility of results\n", "np.random.seed(1)\n", "\n", "for i in range(3):\n", "    print(np.random.choice(scores_A, size=len(scores_A), replace=True))\n", "    print()"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Let's use a bootstrapping approach to perform a one-sample hypothesis test. \n", "\n", "<img src=\"images/westie.jpg\" width=500>\n", "\n", "Imagine we have a sample of ten Westie dog weights in pounds:\n", "```\n", "weights = [11.1, 17.8, 13.5, 11.4, 10.8, 15.4, 16.8, 16.9, 14.4, 11.1]\n", "```\n", "and we want to test if the mean of the population of Westie dog weights was 15.5 lbs, at a significance level of $\\alpha = 0.05$. Unfortunately, we don't have access to more Westies to measure their weights. "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["Set the null and alternative hypotheses for this example. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["weights = [11.1, 17.8, 13.5, 11.4, 10.8, 15.4, 16.8, 16.9, 14.4, 11.1]"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["We need to create the distribution of the mean weights under the null hypothesis using bootstrapping. \n", "\n", "* First, shift the weights so that the mean is equal to the given value, 15.5. We do this to force the null hypothesis to be true when building the distribution of mean weights under the null hypothesis. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["weights_shifted = None "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["* Then, take a large number, say 5,000, bootstrap samples of the shifted weights and compute the mean of the shifted bootstrapped samples to build a distribution of means under the null hypothesis."]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# set n to 5000\n", "n = None \n", "\n", "# initialize an empty array to place the means of the bootstrapped samples\n", "means = None\n", "\n", "#set a random seed for reproducibility\n", "np.random.seed(42) \n", "\n", "for i in range(n):\n", "    boot_weights_shifted = None \n", "    means[i] = np.mean(None)"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["* Finally, count the number of times that we observe mean weights as extreme as the observed mean weight for our sample, and divide by the number of bootstrapped samples to get the p-value. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [], "source": ["# your code here "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["What can you conclude from your results? "]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["# Summary \n", "\n", "* You have used exact permutation tests to perform statistical hypothesis tests. \n", "\n", "* In the case of permutation samples spaces that are too large, you used Monte Carlo simulations to sample from the permutation sample space to perform the hypothesis test. \n", "\n", "* Finally, you learned how to use the bootstrap method of sampling with replacement to create new samples, as well as how to use these new samples to perform simulated statistical hypothesis tests. "]}], "metadata": {"celltoolbar": "Slideshow", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.6"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}}, "nbformat": 4, "nbformat_minor": 2}